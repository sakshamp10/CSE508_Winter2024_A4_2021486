{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saksh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saksh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\saksh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"./Dataset/Reviews.csv\").sample(frac = 0.005, random_state=1)\n",
    "\n",
    "df.dropna(subset=['Text', 'Summary'], inplace=True)\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\[^A-Za-z0-9\\]+', '', str(text))\n",
    "\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    stop = set(stopwords.words('english'))\n",
    "\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['Text'] = df['Text'].apply(clean_text)\n",
    "df['Summary'] = df['Summary'].apply(clean_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288312    love cherry pie lara bar . best tasty bar suga...\n",
       "431726    melitta cafe collection blanc et noir coffee s...\n",
       "110311    girl absolutely loved tuna . heaven could n't ...\n",
       "91855     vendor fast dependable . tea simply best way r...\n",
       "338855    update - 8/9/2010 < br / > lot happen couple m...\n",
       "                                ...                        \n",
       "465528    's taken year lose 64 pound triscuits big part...\n",
       "477980    'm embarrassed admit got suckered 5 star revie...\n",
       "537044     cat love treat . : ) shake pack , come running .\n",
       "51434     bought accident local supermarket . surprise ,...\n",
       "426008    husband love coffee -- think 's best hazelnut ...\n",
       "Name: Text, Length: 2842, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288312                                   cherry pie larabar\n",
       "431726                                       melitta coffee\n",
       "110311                                          great treat\n",
       "91855                                         daily calming\n",
       "338855                              best canned artichoke !\n",
       "                                ...                        \n",
       "465528    triscuits , diet coke weight loss ... perfect ...\n",
       "477980                                      emperor clothes\n",
       "537044                                      absolutely love\n",
       "51434                               new favorite soup ! ! !\n",
       "426008                                           fabulous !\n",
       "Name: Summary, Length: 2842, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ReviewSummaryDataset(Dataset):\n",
    "    def __init__(self, texts, summaries, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.summaries = summaries\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        summary = self.summaries[idx]\n",
    "\n",
    "        # Encode the pairs of text and summaries\n",
    "        encoded_pair = self.tokenizer(text, summary,\n",
    "                                      max_length=self.max_length,\n",
    "                                      truncation=True,\n",
    "                                      padding='max_length',\n",
    "                                      return_tensors='pt')\n",
    "\n",
    "        # These are input_ids and attention_mask from tokenizer output\n",
    "        input_ids = encoded_pair['input_ids'].squeeze()\n",
    "        attention_mask = encoded_pair['attention_mask'].squeeze()\n",
    "\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = ReviewSummaryDataset(train_data['Text'].tolist(), train_data['Summary'].tolist(), tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saksh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.8897305727005005\n",
      "Epoch 0, Loss: 0.7620732188224792\n",
      "Epoch 0, Loss: 0.9272246956825256\n",
      "Epoch 0, Loss: 0.42982128262519836\n",
      "Epoch 0, Loss: 1.1102238893508911\n",
      "Epoch 0, Loss: 0.7465527057647705\n",
      "Epoch 0, Loss: 0.3906932473182678\n",
      "Epoch 0, Loss: 1.5323435068130493\n",
      "Epoch 0, Loss: 0.8804509043693542\n",
      "Epoch 0, Loss: 0.6071149706840515\n",
      "Epoch 0, Loss: 0.9521011114120483\n",
      "Epoch 0, Loss: 1.5150409936904907\n",
      "Epoch 0, Loss: 1.1015095710754395\n",
      "Epoch 0, Loss: 0.3567397892475128\n",
      "Epoch 0, Loss: 1.065793752670288\n",
      "Epoch 0, Loss: 0.7670884728431702\n",
      "Epoch 0, Loss: 0.5085272192955017\n",
      "Epoch 0, Loss: 0.30241021513938904\n",
      "Epoch 0, Loss: 0.6155779957771301\n",
      "Epoch 0, Loss: 0.6609938144683838\n",
      "Epoch 0, Loss: 0.659330427646637\n",
      "Epoch 0, Loss: 1.003301978111267\n",
      "Epoch 0, Loss: 0.7726998329162598\n",
      "Epoch 0, Loss: 0.8593854904174805\n",
      "Epoch 0, Loss: 0.7058529853820801\n",
      "Epoch 0, Loss: 0.5360128283500671\n",
      "Epoch 0, Loss: 0.44785502552986145\n",
      "Epoch 0, Loss: 0.7726408243179321\n",
      "Epoch 0, Loss: 1.175550103187561\n",
      "Epoch 0, Loss: 0.6341785788536072\n",
      "Epoch 0, Loss: 1.196368932723999\n",
      "Epoch 0, Loss: 0.6392630934715271\n",
      "Epoch 0, Loss: 1.2469359636306763\n",
      "Epoch 0, Loss: 0.36243757605552673\n",
      "Epoch 0, Loss: 0.7795578837394714\n",
      "Epoch 0, Loss: 0.7165918946266174\n",
      "Epoch 0, Loss: 0.6948310136795044\n",
      "Epoch 0, Loss: 0.9460825324058533\n",
      "Epoch 0, Loss: 0.4682794213294983\n",
      "Epoch 0, Loss: 0.5548157095909119\n",
      "Epoch 0, Loss: 0.5033707618713379\n",
      "Epoch 0, Loss: 0.9541319012641907\n",
      "Epoch 0, Loss: 0.3679743707180023\n",
      "Epoch 0, Loss: 0.4506112039089203\n",
      "Epoch 0, Loss: 0.6195144653320312\n",
      "Epoch 0, Loss: 0.3902565538883209\n",
      "Epoch 0, Loss: 0.31965160369873047\n",
      "Epoch 0, Loss: 0.40139272809028625\n",
      "Epoch 0, Loss: 0.4968770146369934\n",
      "Epoch 0, Loss: 1.595324158668518\n",
      "Epoch 0, Loss: 0.6234099864959717\n",
      "Epoch 0, Loss: 0.6779239773750305\n",
      "Epoch 0, Loss: 0.536558985710144\n",
      "Epoch 0, Loss: 0.666205883026123\n",
      "Epoch 0, Loss: 0.3785788416862488\n",
      "Epoch 0, Loss: 0.5050328969955444\n",
      "Epoch 0, Loss: 0.38476401567459106\n",
      "Epoch 0, Loss: 0.3956940770149231\n",
      "Epoch 0, Loss: 0.7149257659912109\n",
      "Epoch 0, Loss: 0.7056154012680054\n",
      "Epoch 0, Loss: 0.4011325538158417\n",
      "Epoch 0, Loss: 0.47376328706741333\n",
      "Epoch 0, Loss: 0.9159272313117981\n",
      "Epoch 0, Loss: 0.5683571100234985\n",
      "Epoch 0, Loss: 0.3496822714805603\n",
      "Epoch 0, Loss: 1.6130470037460327\n",
      "Epoch 0, Loss: 0.6309788823127747\n",
      "Epoch 0, Loss: 0.6310532689094543\n",
      "Epoch 0, Loss: 0.7125266790390015\n",
      "Epoch 0, Loss: 0.7729244828224182\n",
      "Epoch 0, Loss: 0.5201661586761475\n",
      "Epoch 0, Loss: 0.4218492805957794\n",
      "Epoch 0, Loss: 0.5262417793273926\n",
      "Epoch 0, Loss: 1.0759103298187256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.8728443384170532\n",
      "Epoch 0, Loss: 1.3296620845794678\n",
      "Epoch 0, Loss: 1.2725915908813477\n",
      "Epoch 0, Loss: 0.34090882539749146\n",
      "Epoch 0, Loss: 0.8621825575828552\n",
      "Epoch 0, Loss: 1.2213082313537598\n",
      "Epoch 0, Loss: 0.6771134734153748\n",
      "Epoch 0, Loss: 0.45646196603775024\n",
      "Epoch 0, Loss: 0.5207059979438782\n",
      "Epoch 0, Loss: 1.512495756149292\n",
      "Epoch 0, Loss: 0.9341555237770081\n",
      "Epoch 0, Loss: 0.6085497140884399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6474109888076782\n",
      "Epoch 0, Loss: 1.7097753286361694\n",
      "Epoch 0, Loss: 0.7227051258087158\n",
      "Epoch 0, Loss: 1.0151420831680298\n",
      "Epoch 0, Loss: 0.8050028085708618\n",
      "Epoch 0, Loss: 0.7631082534790039\n",
      "Epoch 0, Loss: 1.2348319292068481\n",
      "Epoch 0, Loss: 0.6027070879936218\n",
      "Epoch 0, Loss: 0.42905429005622864\n",
      "Epoch 0, Loss: 0.6361421942710876\n",
      "Epoch 0, Loss: 0.5606024265289307\n",
      "Epoch 0, Loss: 0.5195904970169067\n",
      "Epoch 0, Loss: 0.852591335773468\n",
      "Epoch 0, Loss: 0.5479235649108887\n",
      "Epoch 0, Loss: 0.3071536421775818\n",
      "Epoch 0, Loss: 0.9668232202529907\n",
      "Epoch 0, Loss: 0.7878618240356445\n",
      "Epoch 0, Loss: 0.9611565470695496\n",
      "Epoch 0, Loss: 0.37482860684394836\n",
      "Epoch 0, Loss: 0.7207053303718567\n",
      "Epoch 0, Loss: 0.9004090428352356\n",
      "Epoch 0, Loss: 0.3457772135734558\n",
      "Epoch 0, Loss: 0.7521377205848694\n",
      "Epoch 0, Loss: 0.7005221843719482\n",
      "Epoch 0, Loss: 0.3331531286239624\n",
      "Epoch 0, Loss: 0.7282260060310364\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "# Set the pad_token to the eos_token (This is a common practice with GPT models)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Assuming train_data is defined somewhere\n",
    "dataset = ReviewSummaryDataset(train_data['Text'].tolist(), train_data['Summary'].tolist(), tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Training parameters\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "epochs = 4\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for input_ids, attention_mask in dataloader:\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import DataLoader\n",
    "from rouge import Rouge\n",
    "\n",
    "# Assuming the model and tokenizer are already loaded and the test_data prepared\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "class ReviewSummaryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, summaries, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.summaries = summaries\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoded_pair = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoded_pair['input_ids'].squeeze(0)\n",
    "        attention_mask = encoded_pair['attention_mask'].squeeze(0)\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "# Load the test data as an instance of the ReviewSummaryDataset\n",
    "test_dataset = ReviewSummaryDataset(test_data['Text'].tolist(), test_data['Summary'].tolist(), tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "def generate_summaries(model, dataloader):\n",
    "    model.eval()\n",
    "    summaries = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask in dataloader:\n",
    "            outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "            summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            summaries.append(summary)\n",
    "    return summaries\n",
    "\n",
    "# Generate summaries\n",
    "generated_summaries = generate_summaries(model, test_dataloader)\n",
    "\n",
    "# Actual summaries\n",
    "actual_summaries = test_data['Summary'].tolist()\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "def calculate_rouge(hypotheses, references):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hypotheses, references, avg=True)\n",
    "    return scores\n",
    "\n",
    "rouge_scores = calculate_rouge(generated_summaries, actual_summaries)\n",
    "print(\"ROUGE scores:\", rouge_scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
